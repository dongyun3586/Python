{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBpia 사이트 띄우기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome()    \n",
    "driver.get(\"https://www.dbpia.co.kr/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색창에 원하는 키워드 입력하기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=1920,1080')\n",
    "driver = webdriver.Chrome('chromedriver', options=options)    \n",
    "driver.get(\"https://www.dbpia.co.kr/\")\n",
    "\n",
    "# 인공지능 교육 키워드 검색\n",
    "elem = driver.find_element(By.ID, 'keyword')\n",
    "elem.send_keys(\"인공지능 교육\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "\n",
    "# 100개씩 \n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"contents\"]/div[2]/div[2]/div[3]/div[1]/div[2]/div[2]/div[2]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"100\"]').click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100개씩 논문 보기 성공\n",
      "count: 428\n"
     ]
    }
   ],
   "source": [
    "# 검색창에 원하는 키워드 입력하기 - \"인공지능 교육\"\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=1920,1080')\n",
    "driver = webdriver.Chrome('chromedriver', options=options)    \n",
    "driver.get(\"https://www.dbpia.co.kr/\")\n",
    "\n",
    "# csv 파일 생성\n",
    "file_name = 'aipaper.csv'\n",
    "f = open(file_name, 'w', encoding='utf-8-sig', newline='')\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "# 인공지능 교육 키워드 검색\n",
    "elem = driver.find_element(By.ID, 'keyword')\n",
    "elem.send_keys(\"인공지능 교육\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "\n",
    "# 100개씩 보이도록 페이지 세팅\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"contents\"]/div[2]/div[2]/div[3]/div[1]/div[2]/div[2]/div[2]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"100\"]').click()\n",
    "print('100개씩 논문 보기 성공')\n",
    "\n",
    "count = 0\n",
    "# 페이지를 바꿔 가면서 크롤링\n",
    "for page in range(1, 10):\n",
    "    # 로드된 페이지 파싱\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # 논문 정보 추출하기\n",
    "    cont = soup.find('ul', attrs={'id':'dev_search_list'})\n",
    "    rows = cont.find_all('li', attrs={'class':'item'})\n",
    "\n",
    "    for row in rows:\n",
    "        # 논문 제목\n",
    "        title = row.find('h5').a\n",
    "        if title:\n",
    "            link = 'https://www.dbpia.co.kr' + title['href']\n",
    "            title = title.get_text()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 저자\n",
    "        author = row.find('li', attrs={'class':'author'})\n",
    "        if author:\n",
    "            author = author.get_text()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # publisher\n",
    "        publisher = row.find('li', attrs={'class':'publisher'}).get_text()\n",
    "        \n",
    "        # date\n",
    "        date = row.find('li', attrs={'class':'date'}).get_text()\n",
    "        \n",
    "        # 이용수\n",
    "        stats = row.find('small')\n",
    "        if stats:\n",
    "            stats = stats.get_text().replace(',','').strip()\n",
    "        else:\n",
    "            stats = 0\n",
    "        \n",
    "        # print(title, author, publisher, date[:4], link)\n",
    "        # 논문 제목에 인공지능이 없으면 제외시킴\n",
    "        if '인공지능' not in title:\n",
    "            continue\n",
    "        count += 1\n",
    "        csv_writer.writerow([title, date[:4], stats, publisher, author, link])  \n",
    "\n",
    "    # 다음 페이지로 넘어가기 (2 ~ 10)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"pcPaging{}\"]'.format(page+1)).click()\n",
    "\n",
    "print(f'count: {count}')\n",
    "driver.close()\n",
    "f.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100개씩 논문 보기 성공\n",
      "count: 37\n"
     ]
    }
   ],
   "source": [
    "# 검색창에 원하는 키워드 입력하기 - \"머신러닝 교육\"\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=1920,1080')\n",
    "driver = webdriver.Chrome('chromedriver', options=options)    \n",
    "driver.get(\"https://www.dbpia.co.kr/\")\n",
    "\n",
    "# csv 파일 생성\n",
    "file_name = 'aipaper.csv'\n",
    "f = open(file_name, 'w', encoding='utf-8-sig', newline='')\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "# 인공지능 교육 키워드 검색\n",
    "elem = driver.find_element(By.ID, 'keyword')\n",
    "elem.send_keys(\"머신러닝 교육\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "\n",
    "# 100개씩 보이도록 페이지 세팅\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"contents\"]/div[2]/div[2]/div[3]/div[1]/div[2]/div[2]/div[2]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"100\"]').click()\n",
    "print('100개씩 논문 보기 성공')\n",
    "\n",
    "count = 0\n",
    "# 페이지를 바꿔 가면서 크롤링\n",
    "for page in range(1, 2):\n",
    "    # 로드된 페이지 파싱\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # 논문 정보 추출하기\n",
    "    cont = soup.find('ul', attrs={'id':'dev_search_list'})\n",
    "    rows = cont.find_all('li', attrs={'class':'item'})\n",
    "\n",
    "    for row in rows:\n",
    "        # 논문 제목\n",
    "        title = row.find('h5').a\n",
    "        if title:\n",
    "            link = 'https://www.dbpia.co.kr' + title['href']\n",
    "            title = title.get_text()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 저자\n",
    "        author = row.find('li', attrs={'class':'author'})\n",
    "        if author:\n",
    "            author = author.get_text()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # publisher\n",
    "        publisher = row.find('li', attrs={'class':'publisher'}).get_text()\n",
    "        \n",
    "        # date\n",
    "        date = row.find('li', attrs={'class':'date'}).get_text()\n",
    "        \n",
    "        # 이용수\n",
    "        stats = row.find('small')\n",
    "        if stats:\n",
    "            stats = stats.get_text().replace(',','').strip()\n",
    "        else:\n",
    "            stats = 0\n",
    "        \n",
    "        # print(title, author, publisher, date[:4], link)\n",
    "        # 논문 제목에 인공지능이 없으면 제외시킴\n",
    "        if '머신러닝' not in title:\n",
    "            continue\n",
    "        count += 1\n",
    "        csv_writer.writerow([title, date[:4], stats, publisher, author, link])  \n",
    "\n",
    "    # 다음 페이지로 넘어가기 (2 ~ 10)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"pcPaging{}\"]'.format(page+1)).click()\n",
    "\n",
    "print(f'count: {count}')\n",
    "driver.close()\n",
    "f.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fca21ad9a88cea9ca2415063f0b9d5458bbf828e19cdb66b895214e4b1718d2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
