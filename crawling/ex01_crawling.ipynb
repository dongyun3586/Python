{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링과 웹 기본"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 크롤링 기본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "# 1. 라이브러리 임포트\n",
    "import requests                 # 웹 페이지 가져오는 라이브러리\n",
    "from bs4 import BeautifulSoup   # 웹 페이지 분석 라이브러리 \n",
    "\n",
    "# 2. 웹페이지 가져오기 => requests 모듈 사용\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "\n",
    "# 3. 웹페이지 파싱 => BeautifulSoup 모듈 사용\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# 4. 필요한 데이터 추출 => BeautifulSoup 모듈 사용\n",
    "mydata = soup.find('title')  # title 태그의 내용 추출 => 필요한 내용을 가지고 있는 HTML 언어의 일부분을 적어서 추출\n",
    "\n",
    "# 5. 추출한 데이터 활용하기\n",
    "print(mydata.get_text())    # 텍스트만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HTML 페이지에서 필요한 데이터 추출 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''\n",
    "<html> \n",
    "  <body> \n",
    "    <h1 id='title'>[1]크롤링이란?</h1> \n",
    "    <p class='cssstyle'>p1 웹페이지에서 필요한 데이터를 추출하는 것</p> \n",
    "    <p id='p2' align='center'>p2 파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \n",
    "  </body> \n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# p 태그 문장이 두 개인데 이 중에 하나를 선택하려면?\n",
    "# 1. data = soup.find('p', class_='cssstyle')\n",
    "# 2. data = soup.find('p', 'cssstyle')\n",
    "# 3. data = soup.find('p', attrs = {'align': 'center'})\n",
    "# 4. data = soup.find(id='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 웹페이지에서 필요한 데이터를 추출하는 것\n",
      "p1 웹페이지에서 필요한 데이터를 추출하는 것\n",
      "p2 파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "p2 파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('p').get_text())\n",
    "print(soup.find('p', class_='cssstyle').get_text())          # class_= '' => 생략가능(값만 적을 수 있다.)\n",
    "print(soup.find('p', attrs={'align': 'center'}).get_text())  # attrs={:} => 속성과 속성값으로 추출\n",
    "print(soup.find(id='p2').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 웹페이지에서 필요한 데이터를 추출하는 것\n",
      "p2 파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "# find_all() 함수 사용하기\n",
    "paragraph_data = soup.find_all('p')  # 리스트 형태로 반환됨.\n",
    "\n",
    "for paragraph in paragraph_data:\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"cssstyle\">p1 웹페이지에서 필요한 데이터를 추출하는 것</p>,\n",
       " <p align=\"center\" id=\"p2\">p2 파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "titles = soup.find_all('li', 'course')\n",
    "\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "# find()로 가져온 데이터를 find_all()로 다시 추출하기\n",
    "section = soup.find('ul', id='dev_course_list')\n",
    "titles = section.find_all('li', 'course')\n",
    "\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Python의 문자열 함수로 데이터를 원하는 형태로 다듬기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 강사가 실제 사용하는 자동 프로그램 소개\n",
      "2. 필요한 프로그램 설치 시연\n",
      "3. 데이터를 엑셀 파일로 만들기\n",
      "4. 엑셀 파일 이쁘게! 이쁘게!\n",
      "5. 나대신 주기적으로 파이썬 프로그램 실행하기\n",
      "6. 파이썬으로 슬랙(slack) 메신저에 글쓰기\n",
      "7. 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기\n",
      "8. 네이버 API 사용해서, 블로그에 글쓰기\n",
      "9. 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기\n"
     ]
    }
   ],
   "source": [
    "# 3. Python의 문자열 함수로 데이터를 원하는 형태로 다듬기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "section = soup.find('ul', id='dev_course_list')\n",
    "titles = section.find_all('li', 'course')\n",
    "\n",
    "# split() => 특정한 문자를 기준으로 구분된 리스트로 만들기\n",
    "# strip() => 공백 제거\n",
    "# str() => 숫자를 문자열로 만들기\n",
    "for index, title in enumerate(titles):\n",
    "    print(str(index + 1) + '.', title.get_text().split('-')[-1].split('[')[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CSS Selector 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test_css.html')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "items = soup.select('li')                               # 첫번째 데이터만 추출하려면 select_one()\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔재미코딩 크롤링 테스트 페이지 (커리큘럼)\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('html body h1')  # 태그 경로 나열\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "# > 바로 아래 태그 => ul 태그 바로 아래 li 태그\n",
    "items = soup.select('ul > li')  \n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '.클래스명'\n",
    "items = soup.select('.course')\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '#id명'\n",
    "items = soup.select('#start')\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "# '태그.클래스이름1.클래스이름2'\n",
    "items = soup.select('li.course.paid')\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복합 예제 '태그명#id명.클래스명'\n",
    "items = soup.select('ul#hobby_course_list li.course')\n",
    "\n",
    "for item in items:\n",
    "    print (item.get_text())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "# select_one()\n",
    "item = soup.select_one('ul#dev_course_list > li.course.paid')\n",
    "print (item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일정, 커리큘럼 타이틀, 난이도\n",
      "5.1 ~ 6.15, 나만의 엣지있는 블로그 사이트 만들기 (취미로 익히는 IT), 초급\n",
      "6.16 ~ 7.31, 파이썬과 데이터과학 첫걸음 (IT 기본기 익히기), 중급\n"
     ]
    }
   ],
   "source": [
    "# find와 select 비교 => 상호호환 가능(select()와 find_all() 상화 교체 가능)\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_html_css.html')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# tr을 리스트로 가져와 그 안의 td 추출\n",
    "items = soup.select('tr')\n",
    "\n",
    "for item in items:\n",
    "    columns = item.select('td')\n",
    "    row_str = ''\n",
    "\n",
    "    for column in columns:\n",
    "        row_str += ', ' + column.get_text()\n",
    "    print (row_str[2:])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fca21ad9a88cea9ca2415063f0b9d5458bbf828e19cdb66b895214e4b1718d2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
